{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPPjpilNyqAc4nSWMl1Wn77"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Amaç : Belirli bir kelimeyi takip ederek bir sonraki kelimeyi tahmin eden bir model oluşturmak (Boşluk doldurma gibi)"
      ],
      "metadata": {
        "id": "nVbgb7uEc-oa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1-İMPORTİNG LİBRARİES"
      ],
      "metadata": {
        "id": "rybBb_Gwd4tE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "UJLCBFLjd4uI"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2-Get The Data"
      ],
      "metadata": {
        "id": "1QKZLzrZd4u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ],
      "metadata": {
        "id": "yCUVV3pCd4yn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edb3b214-fe80-4fae-a1d2-a1491047f2b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-10 13:11:26--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 142.250.4.139, 142.250.4.113, 142.250.4.101, ...\n",
            "Connecting to drive.google.com (drive.google.com)|142.250.4.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hi0p2eh6o045q4msasjt7mmcrf4tt566/1678453875000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=33957a62-e145-40a7-ad9f-8510de0fa941 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-03-10 13:11:29--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/hi0p2eh6o045q4msasjt7mmcrf4tt566/1678453875000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=33957a62-e145-40a7-ad9f-8510de0fa941\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 142.250.4.132, 2404:6800:4003:c06::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|142.250.4.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M  48.1MB/s    in 1.4s    \n",
            "\n",
            "2023-03-10 13:11:31 (48.1 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VERİ SETİMİZDEKİ 10 ŞARKI SÖZÜNE BAKALIM**"
      ],
      "metadata": {
        "id": "vLJ8EYVjd4zt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3-PREPROCESSİNG\n",
        " 1. Noktalama işaretlerini kaldıracaz\n",
        " 2. Harfleri küçülteceğiz\n",
        " 3. Satır satır yazdıracaz\n"
      ],
      "metadata": {
        "id": "BzVCMTBgd40j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_corpus(corpus,num_words = -1): # corpus : Külliyat\n",
        "  # corpusu tokenize edelim\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words = num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "\n",
        "def create_lyrics_corpus(dataset,field):\n",
        "  # Tüm noktalama işlemlerini kaldıralım\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation),'')  #punctuation : noktalama\n",
        "  # harfleri küçültelim\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # satırlara bölmek için uzun bir dize yapalım\n",
        "  lyrics =dataset[field].str.cat()\n",
        "  corpus = lyrics.split(\"\\n\")\n",
        "  # Sonlarındaki boşlukları kaldıralım\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # boş olan satırları kaldıralım\n",
        "  corpus  = [l for l in corpus if l !='']\n",
        "  return corpus\n"
      ],
      "metadata": {
        "id": "Ay0811fVd43s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datasetimizi okuyalım (10 tane alalım)\n",
        "dataset = pd.read_csv('/tmp/songdata.csv',dtype = str)[:10]\n",
        "# text kısmını alıyoruz\n",
        "corpus = create_lyrics_corpus(dataset,'text')\n",
        "\n",
        "# tokenize corpus \n",
        "tokenizer =tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) +1 \n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcJ7eld7d44f",
        "outputId": "ba166fe6-f70b-4bc8-b3c5-28b514ad1972"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-d62abbe1e732>:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation),'')  #punctuation : noktalama\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4-CREATE SEQUENCES AND LABELS"
      ],
      "metadata": {
        "id": "yYErqDWtd45R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences =[]\n",
        "\n",
        "for line in corpus:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1,len(token_list)):\n",
        "    n_gram_sequences = token_list[:i+1]\n",
        "    sequences.append(n_gram_sequences)\n",
        "\n",
        "\n",
        "\n",
        "## padding işlemleri\n",
        "# en uzun cümlenin kelime sayısını bulalım\n",
        "# padding işlemini en uzun cümleye göre dolduralım\n",
        "\n",
        "max_sequences_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences,maxlen =max_sequences_len,padding='pre')) # 'pre' 0 ları başa koy\n",
        "\n",
        "# girdi ve çıktı olarak ayıralım (input = input_sequences,output = labels)\n",
        "input_sequences,labels = sequences[:,:-1],sequences[:,-1]\n",
        "\n",
        "# Etiketleri 0-1 e  dönüştürelim\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels,num_classes =total_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "_LW_0bDdjkaY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Belirlediğimiz kelimelerin hangi değeri aldığını görelim\n",
        "print(\"Word index is value :\",tokenizer.word_index['know'])\n",
        "print(\"Word index is value :\",tokenizer.word_index['feeling'])\n",
        "# Girdi dizilerinin tokenize şekline bakalım\n",
        "print(\"İnput :\",input_sequences[5])\n",
        "print(\"İnput :\",input_sequences[6])\n",
        "# Obe hot encoding hallerine bakalım\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQuY0NRCjkbJ",
        "outputId": "cafacc70-58e6-4c82-a8dd-51d8462dca0b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word index is value : 32\n",
            "Word index is value : 97\n",
            "İnput : [  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "İnput : [  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5-CREATE AND FİT TEXT GENERATE MODEL\n",
        "  * RNN model yapısına benzer\n",
        "  * loss = 'vategorical_crossentropy' kullanılacak(0 ve 1 den başka değerler bulunduğu için)\n",
        "  "
      ],
      "metadata": {
        "id": "eiGgJ_MMjkcL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequences_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saHOu6NOjke7",
        "outputId": "fa755889-583a-408b-8677-7051a1f4e9f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 15s 76ms/step - loss: 6.0462 - accuracy: 0.0197\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 5.4479 - accuracy: 0.0399\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 5.3603 - accuracy: 0.0444\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 5.2977 - accuracy: 0.0419\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.2199 - accuracy: 0.0404\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 5.1452 - accuracy: 0.0444\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 5.0772 - accuracy: 0.0454\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.0140 - accuracy: 0.0651\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 4.9471 - accuracy: 0.0636\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 4.8780 - accuracy: 0.0843\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 4.7883 - accuracy: 0.0817\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 1s 13ms/step - loss: 4.6893 - accuracy: 0.1014\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 4.5866 - accuracy: 0.0954\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.4780 - accuracy: 0.1125\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.3798 - accuracy: 0.1261\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.2706 - accuracy: 0.1408\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.1729 - accuracy: 0.1604\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.0717 - accuracy: 0.1640\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.9873 - accuracy: 0.1963\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8896 - accuracy: 0.2129\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8077 - accuracy: 0.2326\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.7235 - accuracy: 0.2467\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.6418 - accuracy: 0.2644\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 3.5585 - accuracy: 0.2871\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4884 - accuracy: 0.2916\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4278 - accuracy: 0.3012\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3537 - accuracy: 0.3209\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2898 - accuracy: 0.3300\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2284 - accuracy: 0.3436\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1663 - accuracy: 0.3532\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 3.0983 - accuracy: 0.3562\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0328 - accuracy: 0.3804\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9772 - accuracy: 0.3769\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9328 - accuracy: 0.3840\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.8842 - accuracy: 0.3930\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.8297 - accuracy: 0.4046\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 2.7661 - accuracy: 0.4198\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.7082 - accuracy: 0.4319\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6732 - accuracy: 0.4374\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6209 - accuracy: 0.4475\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5830 - accuracy: 0.4647\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5713 - accuracy: 0.4485\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5184 - accuracy: 0.4667\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5037 - accuracy: 0.4773\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5465 - accuracy: 0.4692\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4824 - accuracy: 0.4702\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 2.4183 - accuracy: 0.4899\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4017 - accuracy: 0.4859\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3411 - accuracy: 0.5010\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2912 - accuracy: 0.5207\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2499 - accuracy: 0.5277\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2415 - accuracy: 0.5272\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2004 - accuracy: 0.5414\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1474 - accuracy: 0.5479\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1248 - accuracy: 0.5600\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0660 - accuracy: 0.5641\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0166 - accuracy: 0.5858\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0038 - accuracy: 0.5797\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.9644 - accuracy: 0.5964\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.9450 - accuracy: 0.6065\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.8994 - accuracy: 0.6150\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.8803 - accuracy: 0.6130\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.8565 - accuracy: 0.6261\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8090 - accuracy: 0.6382\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7808 - accuracy: 0.6438\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.7488 - accuracy: 0.6468\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7199 - accuracy: 0.6498\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6871 - accuracy: 0.6594\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6635 - accuracy: 0.6675\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6407 - accuracy: 0.6690\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6156 - accuracy: 0.6731\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6673 - accuracy: 0.6609\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6658 - accuracy: 0.6569\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6238 - accuracy: 0.6675\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5853 - accuracy: 0.6726\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.5417 - accuracy: 0.6852\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5001 - accuracy: 0.6912\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4772 - accuracy: 0.7033\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.4473 - accuracy: 0.7079\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4227 - accuracy: 0.7129\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4057 - accuracy: 0.7149\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3996 - accuracy: 0.7159\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4039 - accuracy: 0.7104\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3805 - accuracy: 0.7195\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.3604 - accuracy: 0.7210\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.3311 - accuracy: 0.7255\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.3014 - accuracy: 0.7321\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.2871 - accuracy: 0.7361\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2633 - accuracy: 0.7397\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3639 - accuracy: 0.7185\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3630 - accuracy: 0.7104\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2555 - accuracy: 0.7356\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2321 - accuracy: 0.7437\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2031 - accuracy: 0.7513\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1990 - accuracy: 0.7447\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1943 - accuracy: 0.7472\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1553 - accuracy: 0.7578\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1313 - accuracy: 0.7624\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1187 - accuracy: 0.7689\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1052 - accuracy: 0.7639\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.0840 - accuracy: 0.7730\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0894 - accuracy: 0.7674\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1012 - accuracy: 0.7644\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 1.0605 - accuracy: 0.7740\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.0440 - accuracy: 0.7790\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.0249 - accuracy: 0.7805\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 1.0160 - accuracy: 0.7841\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0175 - accuracy: 0.7846\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9980 - accuracy: 0.7866\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9913 - accuracy: 0.7881\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9698 - accuracy: 0.7947\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.9573 - accuracy: 0.7972\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.9458 - accuracy: 0.8012\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9325 - accuracy: 0.8058\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9277 - accuracy: 0.7992\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9304 - accuracy: 0.8027\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9405 - accuracy: 0.8002\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9077 - accuracy: 0.8068\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8878 - accuracy: 0.8128\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8763 - accuracy: 0.8153\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8685 - accuracy: 0.8148\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8539 - accuracy: 0.8234\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8773 - accuracy: 0.8103\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8957 - accuracy: 0.8032\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8829 - accuracy: 0.8073\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8635 - accuracy: 0.8083\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8317 - accuracy: 0.8163\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8115 - accuracy: 0.8199\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8063 - accuracy: 0.8269\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7957 - accuracy: 0.8254\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7929 - accuracy: 0.8290\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7806 - accuracy: 0.8300\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7755 - accuracy: 0.8315\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.7640 - accuracy: 0.8360\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7616 - accuracy: 0.8355\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.7578 - accuracy: 0.8370\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7452 - accuracy: 0.8345\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.7474 - accuracy: 0.8340\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7447 - accuracy: 0.8416\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7352 - accuracy: 0.8305\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7186 - accuracy: 0.8385\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.7097 - accuracy: 0.8456\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6954 - accuracy: 0.8491\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6876 - accuracy: 0.8476\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6798 - accuracy: 0.8481\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6952 - accuracy: 0.8461\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.6914 - accuracy: 0.8451\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6750 - accuracy: 0.8522\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6621 - accuracy: 0.8491\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6626 - accuracy: 0.8532\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6667 - accuracy: 0.8537\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6586 - accuracy: 0.8502\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6546 - accuracy: 0.8517\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6312 - accuracy: 0.8532\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6297 - accuracy: 0.8572\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6413 - accuracy: 0.8491\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6257 - accuracy: 0.8552\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6152 - accuracy: 0.8628\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6263 - accuracy: 0.8602\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.6059 - accuracy: 0.8643\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5968 - accuracy: 0.8663\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5868 - accuracy: 0.8643\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5772 - accuracy: 0.8698\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.5767 - accuracy: 0.8698\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6004 - accuracy: 0.8562\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5851 - accuracy: 0.8633\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5809 - accuracy: 0.8688\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5948 - accuracy: 0.8628\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5813 - accuracy: 0.8703\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5604 - accuracy: 0.8713\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5573 - accuracy: 0.8683\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5438 - accuracy: 0.8769\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5357 - accuracy: 0.8749\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5304 - accuracy: 0.8789\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5255 - accuracy: 0.8789\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5155 - accuracy: 0.8835\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.5155 - accuracy: 0.8845\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5272 - accuracy: 0.8764\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5137 - accuracy: 0.8809\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.5050 - accuracy: 0.8814\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5022 - accuracy: 0.8829\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5004 - accuracy: 0.8845\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4897 - accuracy: 0.8860\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5015 - accuracy: 0.8794\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4848 - accuracy: 0.8819\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4751 - accuracy: 0.8885\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4869 - accuracy: 0.8835\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.4826 - accuracy: 0.8870\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.4771 - accuracy: 0.8855\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4691 - accuracy: 0.8865\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4668 - accuracy: 0.8850\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4790 - accuracy: 0.8850\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4812 - accuracy: 0.8819\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4619 - accuracy: 0.8910\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4516 - accuracy: 0.8875\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4450 - accuracy: 0.8880\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4392 - accuracy: 0.8940\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4394 - accuracy: 0.8910\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4357 - accuracy: 0.8905\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4316 - accuracy: 0.8925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6-Visualize accuracy"
      ],
      "metadata": {
        "id": "xPqOUxGkqIb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "metadata": {
        "id": "XEW9NAeijkfs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "8ba8ee7f-1f49-47a6-a303-ec5f57e011d6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7-Generate new lyrics\n",
        "  * Şimdi modelimizi belirli bir doğrulukla eğittik\n",
        "  * Yeni sözler üretimine başlayacağız\n",
        "  * İlk olarak anahtar bir kelime verecez (seed_text)\n",
        "  * sonrasında metinimizin en fazla ne kadar uzunlukta olması gerektiğini söyleyeceğiz(sonsuzda olabilir).Biz 100 kelime olarak belirleyeceğiz(next_words = 100)"
      ],
      "metadata": {
        "id": "xRfoZpf0jkgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for a in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequences_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "metadata": {
        "id": "0uguBBiOjkvU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33f4fdca-f967-4625-ae91-43c521bd738b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "im feeling chills me to the bone up will cause me strong mad be sailing sailing touch touch night tedious little little little little little little little little little little little little little little little little little little little power am standing just just just just just just just just just just just just just just just too walk too walk too walk too walk eye walls new walk too too eye walk just past past feeling sucker fingers feeling think ride baby think walk darkest life think fingers truth truth final night tedious tough me touch touch tedious little little little touch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wT6pcaW4jlwy"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m2da1vTUjlxr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YTa92F7jjlyj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uf6T70majl2n"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}